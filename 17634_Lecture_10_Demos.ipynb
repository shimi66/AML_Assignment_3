{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6NEofH4wzk-Y"
   },
   "source": [
    "# Topic Modeling\n",
    "We'll use Gensim to model the topics in a corpus of forum posts from a range of different forums."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "NBBxG0n_xK4k"
   },
   "outputs": [],
   "source": [
    "from gensim import models, similarities\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "# Plus a few other assorted inputs.\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aT15MRXu2j4x"
   },
   "source": [
    "## Preprocessing Textual Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "ngRR_d--xhUp"
   },
   "outputs": [],
   "source": [
    "# We'll use a dataset of posts from 20 distinct Usenet forums.\n",
    "texts = fetch_20newsgroups(remove=('headers', 'footers', 'quotes'))   # Strip metadata that makes the problem too easy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "MtvRkZN1xp2o"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Do', 'you', 'have', 'Weitek', 's', 'address', 'phone', 'number', 'I', 'd']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We'd typically start by tokenizing the data \n",
    "from gensim.utils import tokenize\n",
    "\n",
    "tokenized_texts = [list(tokenize(text)) for text in texts.data]\n",
    "tokenized_texts[3][:10]  # Print a random example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "9qe0zFuR3_Ew"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['do', 'you', 'have', 'weitek', 's', 'address', 'phone', 'number', 'i', 'd']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And then stem all words.\n",
    "from gensim.parsing.porter import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_texts = [[stemmer.stem(word) for word in text] for text in tokenized_texts]\n",
    "stemmed_texts[3][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "5Lm38fkoz3yR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pattern in c:\\users\\erik\\anaconda3\\lib\\site-packages (3.6)\n",
      "Requirement already satisfied: scipy in c:\\users\\erik\\anaconda3\\lib\\site-packages (from pattern) (1.9.1)\n",
      "Requirement already satisfied: pdfminer.six in c:\\users\\erik\\anaconda3\\lib\\site-packages (from pattern) (20221105)\n",
      "Requirement already satisfied: future in c:\\users\\erik\\anaconda3\\lib\\site-packages (from pattern) (0.18.2)\n",
      "Requirement already satisfied: feedparser in c:\\users\\erik\\anaconda3\\lib\\site-packages (from pattern) (6.0.10)\n",
      "Requirement already satisfied: numpy in c:\\users\\erik\\anaconda3\\lib\\site-packages (from pattern) (1.21.5)\n",
      "Requirement already satisfied: lxml in c:\\users\\erik\\anaconda3\\lib\\site-packages (from pattern) (4.9.1)\n",
      "Requirement already satisfied: requests in c:\\users\\erik\\anaconda3\\lib\\site-packages (from pattern) (2.28.1)\n",
      "Requirement already satisfied: backports.csv in c:\\users\\erik\\anaconda3\\lib\\site-packages (from pattern) (1.0.7)\n",
      "Requirement already satisfied: cherrypy in c:\\users\\erik\\anaconda3\\lib\\site-packages (from pattern) (18.8.0)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\erik\\anaconda3\\lib\\site-packages (from pattern) (4.11.1)\n",
      "Requirement already satisfied: nltk in c:\\users\\erik\\anaconda3\\lib\\site-packages (from pattern) (3.7)\n",
      "Requirement already satisfied: mysqlclient in c:\\users\\erik\\anaconda3\\lib\\site-packages (from pattern) (2.1.1)\n",
      "Requirement already satisfied: python-docx in c:\\users\\erik\\anaconda3\\lib\\site-packages (from pattern) (0.8.11)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\erik\\anaconda3\\lib\\site-packages (from beautifulsoup4->pattern) (2.3.1)\n",
      "Requirement already satisfied: portend>=2.1.1 in c:\\users\\erik\\anaconda3\\lib\\site-packages (from cherrypy->pattern) (3.1.0)\n",
      "Requirement already satisfied: more-itertools in c:\\users\\erik\\anaconda3\\lib\\site-packages (from cherrypy->pattern) (9.1.0)\n",
      "Requirement already satisfied: zc.lockfile in c:\\users\\erik\\anaconda3\\lib\\site-packages (from cherrypy->pattern) (3.0)\n",
      "Requirement already satisfied: pywin32>=227 in c:\\users\\erik\\anaconda3\\lib\\site-packages (from cherrypy->pattern) (302)\n",
      "Requirement already satisfied: cheroot>=8.2.1 in c:\\users\\erik\\anaconda3\\lib\\site-packages (from cherrypy->pattern) (9.0.0)\n",
      "Requirement already satisfied: jaraco.collections in c:\\users\\erik\\anaconda3\\lib\\site-packages (from cherrypy->pattern) (3.8.0)\n",
      "Requirement already satisfied: sgmllib3k in c:\\users\\erik\\anaconda3\\lib\\site-packages (from feedparser->pattern) (1.0.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\erik\\anaconda3\\lib\\site-packages (from nltk->pattern) (1.1.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\erik\\anaconda3\\lib\\site-packages (from nltk->pattern) (4.64.1)\n",
      "Requirement already satisfied: click in c:\\users\\erik\\anaconda3\\lib\\site-packages (from nltk->pattern) (8.0.4)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\erik\\anaconda3\\lib\\site-packages (from nltk->pattern) (2022.7.9)\n",
      "Requirement already satisfied: charset-normalizer>=2.0.0 in c:\\users\\erik\\anaconda3\\lib\\site-packages (from pdfminer.six->pattern) (2.0.4)\n",
      "Requirement already satisfied: cryptography>=36.0.0 in c:\\users\\erik\\anaconda3\\lib\\site-packages (from pdfminer.six->pattern) (37.0.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\erik\\anaconda3\\lib\\site-packages (from requests->pattern) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\erik\\anaconda3\\lib\\site-packages (from requests->pattern) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\erik\\anaconda3\\lib\\site-packages (from requests->pattern) (2022.9.14)\n",
      "Requirement already satisfied: jaraco.functools in c:\\users\\erik\\anaconda3\\lib\\site-packages (from cheroot>=8.2.1->cherrypy->pattern) (3.6.0)\n",
      "Requirement already satisfied: six>=1.11.0 in c:\\users\\erik\\anaconda3\\lib\\site-packages (from cheroot>=8.2.1->cherrypy->pattern) (1.16.0)\n",
      "Requirement already satisfied: cffi>=1.12 in c:\\users\\erik\\anaconda3\\lib\\site-packages (from cryptography>=36.0.0->pdfminer.six->pattern) (1.15.1)\n",
      "Requirement already satisfied: tempora>=1.8 in c:\\users\\erik\\anaconda3\\lib\\site-packages (from portend>=2.1.1->cherrypy->pattern) (5.2.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\erik\\anaconda3\\lib\\site-packages (from click->nltk->pattern) (0.4.5)\n",
      "Requirement already satisfied: jaraco.classes in c:\\users\\erik\\anaconda3\\lib\\site-packages (from jaraco.collections->cherrypy->pattern) (3.2.3)\n",
      "Requirement already satisfied: jaraco.text in c:\\users\\erik\\anaconda3\\lib\\site-packages (from jaraco.collections->cherrypy->pattern) (3.11.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\erik\\anaconda3\\lib\\site-packages (from zc.lockfile->cherrypy->pattern) (63.4.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\erik\\anaconda3\\lib\\site-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six->pattern) (2.21)\n",
      "Requirement already satisfied: pytz in c:\\users\\erik\\anaconda3\\lib\\site-packages (from tempora>=1.8->portend>=2.1.1->cherrypy->pattern) (2022.1)\n",
      "Requirement already satisfied: autocommand in c:\\users\\erik\\anaconda3\\lib\\site-packages (from jaraco.text->jaraco.collections->cherrypy->pattern) (2.2.2)\n",
      "Requirement already satisfied: inflect in c:\\users\\erik\\anaconda3\\lib\\site-packages (from jaraco.text->jaraco.collections->cherrypy->pattern) (6.0.2)\n",
      "Requirement already satisfied: jaraco.context>=4.1 in c:\\users\\erik\\anaconda3\\lib\\site-packages (from jaraco.text->jaraco.collections->cherrypy->pattern) (4.3.0)\n",
      "Requirement already satisfied: pydantic>=1.9.1 in c:\\users\\erik\\anaconda3\\lib\\site-packages (from inflect->jaraco.text->jaraco.collections->cherrypy->pattern) (1.10.5)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\erik\\anaconda3\\lib\\site-packages (from pydantic>=1.9.1->inflect->jaraco.text->jaraco.collections->cherrypy->pattern) (4.3.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Erik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# While this gets us useful data by itself, we'll instead lemmatize the documents\n",
    "# using another library. We'll need to do a bit of setup to make this work.\n",
    "!pip install pattern\n",
    "import nltk\n",
    "nltk.download('omw-1.4')\n",
    "# from gensim.utils import lemmatize\n",
    "from pattern.en import lemma\n",
    "# from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "# docs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "QUXW2xgF0j4c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['a'], ['f', 'a', 'i', 'r'], ['n', 'u', 'm', 'b', 'e', 'r'], ['o', 'f'], ['b', 'r', 'a', 'v', 'e'], ['s', 'o', 'u', 'l'], ['w', 'h', 'o'], ['u', 'p', 'g', 'r', 'a', 'd', 'e'], ['t', 'h', 'e', 'i', 'r'], ['s', 'i'], ['c', 'l', 'o', 'c', 'k'], ['o', 's', 'c', 'i', 'l', 'l', 'a', 't', 'o', 'r'], ['h', 'a', 'v', 'e'], ['s', 'h', 'a', 'r', 'e'], ['t', 'h', 'e', 'i', 'r'], ['e', 'x', 'p', 'e', 'r', 'i', 'e', 'n', 'c', 'e'], ['f', 'o', 'r'], ['t', 'h', 'i'], ['p', 'o', 'l', 'l', '.'], ['p', 'l', 'e', 'a', 's', 'e'], ['s', 'e', 'n', 'd'], ['a'], ['b', 'r', 'i', 'e', 'f'], ['m', 'e', 's', 's', 'a', 'g', 'e'], ['d', 'e', 't', 'a', 'i', 'l'], ['y', 'o', 'u', 'r'], ['e', 'x', 'p', 'e', 'r', 'i', 'e', 'n', 'c', 'e'], ['w', 'i', 't', 'h'], ['t', 'h', 'e'], ['p', 'r', 'o', 'c', 'e', 'd', 'u', 'r', 'e', '.'], ['t', 'o', 'p'], ['s', 'p', 'e', 'e', 'd'], ['a', 't', 't', 'a', 'i', 'n', 'e', 'd', ','], ['c', 'p', 'u'], ['r', 'a', 't', 'e'], ['s', 'p', 'e', 'e', 'd', ','], ['a', 'd', 'd'], ['o', 'n'], ['c', 'a', 'r', 'd'], ['a', 'n', 'd'], ['a', 'd', 'a', 'p', 't', 'e', 'r', 's', ','], ['h', 'e', 'a', 't'], ['s', 'i', 'n', 'k', 's', ','], ['h', 'o', 'u', 'r'], ['o', 'f'], ['u', 's', 'a', 'g', 'e'], ['p', 'e', 'r'], ['d', 'a', 'y', ','], ['f', 'l', 'o', 'p', 'p', 'y'], ['d', 'i', 's', 'k'], ['f', 'u', 'n', 'c', 't', 'i', 'o', 'n', 'a', 'l', 'i', 't', 'y'], ['w', 'i', 't', 'h'], ['8', '0', '0'], ['a', 'n', 'd'], ['1', '.', '4'], ['m'], ['f', 'l', 'o', 'p', 'p', 'y'], ['b', 'e'], ['e', 's', 'p', 'e', 'c', 'i', 'a', 'l', 'l', 'y'], ['r', 'e', 'q', 'u', 'e', 's', 't', 'e', 'd', '.'], ['i'], ['w', 'i', 'l', 'l'], ['b', 'e'], ['s', 'u', 'm', 'm', 'a', 'r', 'i', 'z', 'e'], ['i', 'n'], ['t', 'h', 'e'], ['n', 'e', 'x', 't'], ['t', 'w', 'o'], ['d', 'a', 'y', 's', ','], ['s', 'o'], ['p', 'l', 'e', 'a', 's', 'e'], ['a', 'd', 'd'], ['t', 'o'], ['t', 'h', 'e'], ['n', 'e', 't', 'w', 'o', 'r', 'k'], ['k', 'n', 'o', 'w', 'l', 'e', 'd', 'g', 'e'], ['b', 'a', 's', 'e'], ['i', 'f'], ['y', 'o', 'u'], ['h', 'a', 'v', 'e'], ['d', 'o'], ['t', 'h', 'e'], ['c', 'l', 'o', 'c', 'k'], ['u', 'p', 'g', 'r', 'a', 'd', 'e'], ['a', 'n', 'd'], ['h', 'a', 'v', 'e'], ['a', 'n', 's', 'w', 'e', 'r'], ['t', 'h', 'i'], ['p', 'o', 'l', 'l', '.'], ['t', 'h', 'a', 'n', 'k', 's', '.']]\n"
     ]
    }
   ],
   "source": [
    "# Here we start from full sentences again to help the NLP library understand the types of words. \n",
    "# Note: this can take quite a while!\n",
    "# print(texts.data[0])\n",
    "# print(lemma(texts.data[0]))  # For some reason I occasionally have to run this several times before things start working.\n",
    "lemmatized_texts = [[list(lemma(word)) for word in text.split()] for text in texts.data]\n",
    "\n",
    "print(lemmatized_texts[1])\n",
    "# lemmatized_texts[3][:10]  # Print a random example.\n",
    "\n",
    "# from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "# lemmatized_texts = [list(lemmatizer.lemmatize(text)) for text in texts.data]\n",
    "# lemmatized_texts[3]  # Print a random example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "_ipxM0N0xQ8v"
   },
   "outputs": [],
   "source": [
    "# Create a corpus from a list of texts.\n",
    "# The dictionary just extracts and numbers each distinct word.\n",
    "dictionary = Dictionary(lemmatized_texts)\n",
    "# A corpus is a sparse datastore containing the number of times each word appears in each document.\n",
    "corpus = [dictionary.doc2bow(text) for text in lemmatized_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "ieBsMmsUzihf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i be wonder if anyone out there can enlighten me on thi car i see the other day. it be a 2-door sport car, look to be from the late 60s/ early 70s. it be call a bricklin. the door be really small. in addition, the front bumper be separate from the rest of the body. thi be all i know. if anyone can tellme a model name, engine specs, year of production, where thi car be made, history, or whatever info you have on thi funky look car, please e-mail.   \tindex: 0\tcount: 1\n"
     ]
    }
   ],
   "source": [
    "# Print a sample of dictionary items.\n",
    "top_words_in_doc_0 = sorted(corpus[0], key=lambda e: e[1], reverse=True)[:10]\n",
    "for word_index, count in top_words_in_doc_0:\n",
    "  print(f'{dictionary[word_index]}   \\tindex: {word_index}\\tcount: {count:,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cf5fKgmq6UWR"
   },
   "source": [
    "## Models Trained on Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "_xj6G_Nm6WAc"
   },
   "outputs": [],
   "source": [
    "# Let's start with a simple TF-IDF model. Worth noting: SKLearn also has a Tfidf vectorizer.\n",
    "model = models.TfidfModel(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "G4o_V5kl6w8L"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    \tindex: 1\ttf-idf: 0.39\n",
      "0   \tindex: 6\ttf-idf: 0.31\n",
      "7   \tindex: 9\ttf-idf: 0.26\n",
      "/   \tindex: 5\ttf-idf: 0.23\n",
      "e   \tindex: 17\ttf-idf: 0.23\n",
      ",   \tindex: 2\ttf-idf: 0.23\n",
      "6   \tindex: 8\ttf-idf: 0.22\n",
      "I   \tindex: 11\ttf-idf: 0.22\n",
      "o   \tindex: 26\ttf-idf: 0.19\n",
      "a   \tindex: 13\ttf-idf: 0.18\n"
     ]
    }
   ],
   "source": [
    "# What are the most _salient_ words in document 0?\n",
    "top_tfidf_words = sorted(model[corpus[0]], key=lambda e: e[1], reverse=True)[:10]\n",
    "for word_index, score in top_tfidf_words:\n",
    "  print(f'{dictionary[word_index]}   \\tindex: {word_index}\\ttf-idf: {score:.2f}')\n",
    "\n",
    "# What's different from the previous ranking? What do you think the topic label might be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vlQrCIvd7QGD"
   },
   "outputs": [],
   "source": [
    "print(texts.target_names[texts.target[0]])  # 7 = 'rec.autos'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fSTedgQY9pjz"
   },
   "outputs": [],
   "source": [
    "# We could conver this to a giant vector spanning all documents and words.\n",
    "# It'll be mostly zeros, of course.\n",
    "tfidf_vector = np.zeros([len(texts.target), len(dictionary)])\n",
    "for doc_idx, doc in enumerate(corpus):\n",
    "  for word_ix, word_tfidf in model[doc]:\n",
    "    tfidf_vector[doc_idx, word_ix] = word_tfidf\n",
    "tfidf_vector.shape  # How many entries are there in this vector? What % do you guess is zero?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ePkxDwFHCRGz"
   },
   "outputs": [],
   "source": [
    "print(f'Percentage of 0 entries: {(tfidf_vector == 0).sum() / tfidf_vector.size:.3%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AJfsP5o176Ok"
   },
   "outputs": [],
   "source": [
    "# Then we can train a model that predicts the topic from these values.\n",
    "# A quick tangent: we'd normally use sklearn.feature_extraction.text.TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "clf = MultinomialNB(alpha=.01)\n",
    "clf.fit(tfidf_vector, texts.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YPuwmdMZAm_E"
   },
   "outputs": [],
   "source": [
    "# Let's index our test data in the same way and test the model.\n",
    "test_texts, test_targets = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'), return_X_y=True)\n",
    "# We'll save ourselves a bit of time by focusing on 1K documents.\n",
    "test_texts = test_texts[:1000]\n",
    "test_targets = test_targets[:1000]\n",
    "\n",
    "# Lemmatize text, index with the same dictionary, and create a TF-IDF vector.\n",
    "test_lemmatized = [list(lemmatize(text)) for text in test_texts]\n",
    "test_corpus = [dictionary.doc2bow(text) for text in test_lemmatized]\n",
    "test_tfidf_vector = np.zeros([len(test_targets), len(dictionary)])\n",
    "for doc_idx, doc in enumerate(test_corpus):\n",
    "  for word_ix, word_tfidf in model[doc]:\n",
    "    test_tfidf_vector[doc_idx, word_ix] = word_tfidf\n",
    "\n",
    "print(f'Accuracy: {clf.score(test_tfidf_vector, test_targets):.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pp8j6r5PCpQK"
   },
   "outputs": [],
   "source": [
    "# What are the most important features for each category?\n",
    "for ix, category in enumerate(texts.target_names):\n",
    "    top10 = np.argsort(clf.coef_[ix])[-10:]\n",
    "    print(f'{category}:   \\t{\" \".join([dictionary[index] for index in top10])}')\n",
    "# Pretty reasonable for the most part, but note the first few in almost every category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ntjeWXS7yTKd"
   },
   "outputs": [],
   "source": [
    "# Okay, let's do something more exciting: build an LDA model.\n",
    "# Note: we could also do this with SKLearn using LinearDiscriminantAnalysis.\n",
    "\n",
    "# Let's start with 20 topics.\n",
    "num_topics = 20\n",
    "model = models.LdaMulticore(corpus, num_topics=num_topics, id2word=dictionary, passes=10, workers=4, dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QgjjmkzD7yyi"
   },
   "outputs": [],
   "source": [
    "# Let's again look at the most salient words per topic, but note that we no longer\n",
    "# have \"labels\" now.\n",
    "for ix in range(num_topics):\n",
    "  top10 = np.argsort(model.get_topics()[ix])[-10:]\n",
    "  print(f'{ix}:  {\" \".join([dictionary[index] for index in top10])}')  # See any patterns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s6fpoQghHGCh"
   },
   "outputs": [],
   "source": [
    "# Is this useful? One thing you can now do is find relevant neighbors for a given document\n",
    "def topics_to_vector(topics):\n",
    "  \"\"\"Little util method for creating vectors.\"\"\"\n",
    "  vector = np.zeros(num_topics)\n",
    "  for index, score in topics:\n",
    "    vector[index] = score\n",
    "  return vector\n",
    "\n",
    "# Print the first document with its category.\n",
    "print('Label:', texts.target_names[texts.target[0]])\n",
    "print(texts.data[0])\n",
    "print('\\n\\n')\n",
    "\n",
    "# Conver to a vector and find nearest neighbors.\n",
    "min_dist = 1e9\n",
    "nearest_neighbor = -1\n",
    "topic_vector = topics_to_vector(model.get_document_topics(corpus[0]))\n",
    "print(topic_vector)\n",
    "for ix, doc in enumerate(corpus[1:], start=1):\n",
    "  doc_vector = topics_to_vector(model.get_document_topics(doc))\n",
    "  dist = np.linalg.norm(topic_vector - doc_vector)\n",
    "  if dist < min_dist:\n",
    "    min_dist = dist\n",
    "    nearest_neighbor = ix\n",
    "    print(f'New nearest neighbor: {ix} with distance {dist:.4f} and label {texts.target_names[texts.target[ix]]}')\n",
    "\n",
    "print('\\n\\nNearest neighbor: ')\n",
    "print('Label:', texts.target_names[texts.target[nearest_neighbor]])\n",
    "print(texts.data[nearest_neighbor])\n",
    "print(topics_to_vector(model.get_document_topics(corpus[nearest_neighbor])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5AZW_u606Cdm"
   },
   "outputs": [],
   "source": [
    "# How coherent are these topics?\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "cm = CoherenceModel(model=model, texts=lemmatized_texts, dictionary=dictionary, coherence='u_mass')\n",
    "coherence = cm.get_coherence()\n",
    "coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oRjMvC9t6wBj"
   },
   "outputs": [],
   "source": [
    "# How about HAC?\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "cluster = AgglomerativeClustering(n_clusters=20, affinity='euclidean', linkage='single', compute_distances=True)\n",
    "cluster.fit(tfidf_vector[:2000])  # Note: this takes a very long time, so just using a small subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rZMDuLbe7NbU"
   },
   "outputs": [],
   "source": [
    "# Let's visualize the clusters.\n",
    "# Using code from https://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_dendrogram.html#sphx-glr-auto-examples-cluster-plot-agglomerative-dendrogram-py\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "\n",
    "\n",
    "def plot_dendrogram(model, **kwargs):\n",
    "    \"\"\"Create linkage matrix and then plot the dendrogram.\"\"\"\n",
    "\n",
    "    # create the counts of samples under each node\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    linkage_matrix = np.column_stack(\n",
    "        [model.children_, model.distances_, counts]\n",
    "    ).astype(float)\n",
    "\n",
    "    # Plot the corresponding dendrogram\n",
    "    dendrogram(linkage_matrix, **kwargs)\n",
    "\n",
    "# setting distance_threshold=0 ensures we compute the full tree.\n",
    "plt.title(\"Hierarchical Clustering Dendrogram\")\n",
    "# plot the top three levels of the dendrogram\n",
    "plot_dendrogram(cluster, truncate_mode=\"level\", p=20)\n",
    "plt.xlabel(\"Number of points in node (or index of point if no parenthesis).\")\n",
    "plt.show()\n",
    "# Not very informative; might just require more samples."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
