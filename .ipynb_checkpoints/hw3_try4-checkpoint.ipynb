{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the one star and five star parts of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load in the compressed file to an iterator object\n",
    "import gzip\n",
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim import corpora\n",
    "import multiprocessing\n",
    "\n",
    "\n",
    "def parse(file_path):\n",
    "    with gzip.open(file_path, 'r') as f:\n",
    "        for compressed_json in f:\n",
    "            obj = json.loads(compressed_json.decode('utf-8'))\n",
    "            yield obj\n",
    "\n",
    "path = 'amazon/Home_and_Kitchen.json.gz'\n",
    "it = parse(path)\n",
    "counter1 = 0\n",
    "counter2 = 0\n",
    "\n",
    "def extract_some(it, output_file1, output_file2):\n",
    "    global counter1, counter2\n",
    "    # Open the output file for writing\n",
    "    with open(output_file1, 'w', newline='') as csvfile1:\n",
    "        with open(output_file2, 'w', newline='') as csvfile2:\n",
    "            fieldnames = ['reviewText']\n",
    "            writer1 = csv.DictWriter(csvfile1, fieldnames=fieldnames)\n",
    "            writer2 = csv.DictWriter(csvfile2, fieldnames=fieldnames)\n",
    "            writer1.writeheader()\n",
    "            writer2.writeheader()\n",
    "            for obj in it:\n",
    "                if counter1 > 1000000 and counter2 < 1000000:\n",
    "                    break\n",
    "                # With a 10% probability, write the 'asin' and 'reviewText' fields to the output file\n",
    "                if 'reviewText' in obj and 'overall' in obj:\n",
    "                    if obj.get('overall') == 1:\n",
    "                        if counter1 > 1000000:\n",
    "                            continue\n",
    "                        row = {'reviewText': obj.get('reviewText')}\n",
    "                        writer1.writerow(row)\n",
    "                        counter1+=1\n",
    "                    elif obj.get('overall') == 5:\n",
    "                        if counter2 > 1000000:\n",
    "                            continue\n",
    "                        row = {'reviewText': obj.get('reviewText')}\n",
    "                        writer2.writerow(row)\n",
    "                        counter2+=1\n",
    "\n",
    "output_file1 = 'overall1.csv'\n",
    "output_file2 = 'overall5.csv'\n",
    "extract_some(it, output_file1, output_file2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run LDA on one star"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Erik\\AppData\\Local\\Temp\\ipykernel_31696\\3168729366.py:7: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data = pd.read_csv(output_file, error_bad_lines=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score u_mass:  -3.144784836462201\n",
      "\n",
      "Coherence Score c_v:  0.5336594155576114\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "TextIOWrapper.write() takes exactly one argument (2 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_31696\\3168729366.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     47\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopic\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlda_model_rating1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_topics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m         \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Topic: {} \\nWords: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopic\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 49\u001b[1;33m         \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\nCoherence Score u_mass: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     50\u001b[0m         \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'\\nCoherence Score c_v: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc_1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: TextIOWrapper.write() takes exactly one argument (2 given)"
     ]
    }
   ],
   "source": [
    "# Define stopwords\n",
    "stop_words = set(STOPWORDS)\n",
    "\n",
    "output_file = output_file1\n",
    "\n",
    "# Load data from CSV file\n",
    "data = pd.read_csv(output_file, error_bad_lines=False)\n",
    "\n",
    "# Defined stopwords from above\n",
    "\n",
    "# Define a function to tokenize and preprocess the data\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in simple_preprocess(text):\n",
    "        if token not in stop_words and len(token) > 3:\n",
    "            result.append(token)\n",
    "    return result\n",
    "\n",
    "# Tokenize and preprocess the data\n",
    "data_processed = data['reviewText'].astype(str).apply(preprocess)\n",
    "\n",
    "# Create dictionary and corpus\n",
    "dictionary = corpora.Dictionary(data_processed)\n",
    "corpus = [dictionary.doc2bow(text) for text in data_processed]\n",
    "\n",
    "# Train the LDA model\n",
    "lda_model_rating1 = gensim.models.ldamulticore.LdaMulticore(corpus=corpus,\n",
    "                                                id2word=dictionary,\n",
    "                                                num_topics=10, \n",
    "                                                random_state=42,\n",
    "                                                passes=10,\n",
    "                                                per_word_topics=True,\n",
    "                                                workers=20)\n",
    "\n",
    "# Print the topics\n",
    "\n",
    "# Compute coherence score\n",
    "cm = CoherenceModel(model=lda_model_rating1, texts=data_processed, dictionary=dictionary, coherence='u_mass')\n",
    "c_1 = cm.get_coherence()\n",
    "print('\\nCoherence Score u_mass: ', c_1)\n",
    "cm = CoherenceModel(model=lda_model_rating1, texts=data_processed, dictionary=dictionary, coherence='c_v')\n",
    "c_1 = cm.get_coherence()\n",
    "print('\\nCoherence Score c_v: ', c_1)\n",
    "\n",
    "#write the topics and coherences to a txt file\n",
    "with open('topics_rating1.txt', 'w') as f:\n",
    "    for idx, topic in lda_model_rating1.print_topics(-1):\n",
    "        f.write('Topic: {} \\nWords: {}'.format(idx, topic))\n",
    "        f.write('\\nCoherence Score u_mass: ' + str(c_1))\n",
    "        f.write('\\nCoherence Score c_v: ' +  str(c_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Erik\\AppData\\Local\\Temp\\ipykernel_31696\\3310735123.py:4: FutureWarning: The error_bad_lines argument has been deprecated and will be removed in a future version. Use on_bad_lines in the future.\n",
      "\n",
      "\n",
      "  data = pd.read_csv(output_file, error_bad_lines=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score u_mass:  -10.499066227330939\n",
      "\n",
      "Coherence Score c_v:  0.4863318360576434\n"
     ]
    }
   ],
   "source": [
    "output_file = output_file2\n",
    "\n",
    "# Load data from CSV file\n",
    "data = pd.read_csv(output_file, error_bad_lines=False)\n",
    "\n",
    "# Tokenize and preprocess the data\n",
    "data_processed = data['reviewText'].astype(str).apply(preprocess)\n",
    "\n",
    "# Create dictionary and corpus\n",
    "dictionary = corpora.Dictionary(data_processed)\n",
    "corpus = [dictionary.doc2bow(text) for text in data_processed]\n",
    "\n",
    "# Train the LDA model\n",
    "lda_model_rating2 = gensim.models.ldamulticore.LdaMulticore(corpus=corpus,\n",
    "                                                id2word=dictionary,\n",
    "                                                num_topics=10, \n",
    "                                                random_state=42,\n",
    "                                                passes=10,\n",
    "                                                per_word_topics=True,\n",
    "                                                workers=20)\n",
    "\n",
    "# Print the topics\n",
    "\n",
    "# Compute coherence score\n",
    "cm = CoherenceModel(model=lda_model_rating1, texts=data_processed, dictionary=dictionary, coherence='u_mass')\n",
    "c_1 = cm.get_coherence()\n",
    "print('\\nCoherence Score u_mass: ', c_1)\n",
    "cm = CoherenceModel(model=lda_model_rating2, texts=data_processed, dictionary=dictionary, coherence='c_v')\n",
    "c_1 = cm.get_coherence()\n",
    "print('\\nCoherence Score c_v: ', c_1)\n",
    "\n",
    "#write the topics and coherences to a txt file\n",
    "with open('topics_rating5.txt', 'w') as f:\n",
    "    for idx, topic in lda_model_rating2.print_topics(-1):\n",
    "        f.write('Topic: {} \\nWords: {}'.format(idx, topic))\n",
    "        f.write('\\nCoherence Score u_mass: ' + str(c_1))\n",
    "        f.write('\\nCoherence Score c_v: ' + str(c_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "7b58790328ddf65240d6cee0f2f71685590be9dd6763f1e859d5204810173800"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
