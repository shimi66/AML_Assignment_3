{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6NEofH4wzk-Y"
   },
   "source": [
    "# Topic Modeling\n",
    "We'll use Gensim to model the topics in a corpus of forum posts from a range of different forums."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "NBBxG0n_xK4k"
   },
   "outputs": [],
   "source": [
    "from gensim import models, similarities\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "# Plus a few other assorted inputs.\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aT15MRXu2j4x"
   },
   "source": [
    "## Preprocessing Textual Corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ngRR_d--xhUp"
   },
   "outputs": [],
   "source": [
    "# We'll use a dataset of posts from 20 distinct Usenet forums.\n",
    "texts = fetch_20newsgroups(remove=('headers', 'footers', 'quotes'))   # Strip metadata that makes the problem too easy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "MtvRkZN1xp2o"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Do', 'you', 'have', 'Weitek', 's', 'address', 'phone', 'number', 'I', 'd']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We'd typically start by tokenizing the data \n",
    "from gensim.utils import tokenize\n",
    "\n",
    "tokenized_texts = [list(tokenize(text)) for text in texts.data]\n",
    "tokenized_texts[3][:10]  # Print a random example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "9qe0zFuR3_Ew"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['do', 'you', 'have', 'weitek', 's', 'address', 'phone', 'number', 'i', 'd']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And then stem all words.\n",
    "from gensim.parsing.porter import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_texts = [[stemmer.stem(word) for word in text] for text in tokenized_texts]\n",
    "stemmed_texts[3][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "5Lm38fkoz3yR"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'lemmatize' from 'gensim.utils' (C:\\Users\\Erik\\anaconda3\\lib\\site-packages\\gensim\\utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [10]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# While this gets us useful data by itself, we'll instead lemmatize the documents\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# using another library. We'll need to do a bit of setup to make this work.\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# !pip install pattern\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# import nltk\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# nltk.download('omw-1.4')\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgensim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lemmatize\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'lemmatize' from 'gensim.utils' (C:\\Users\\Erik\\anaconda3\\lib\\site-packages\\gensim\\utils.py)"
     ]
    }
   ],
   "source": [
    "# While this gets us useful data by itself, we'll instead lemmatize the documents\n",
    "# using another library. We'll need to do a bit of setup to make this work.\n",
    "!pip install pattern\n",
    "import nltk\n",
    "nltk.download('omw-1.4')\n",
    "from gensim.utils import lemmatize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "QUXW2xgF0j4c"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lemmatize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Here we start from full sentences again to help the NLP library understand the types of words. \u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Note: this can take quite a while!\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mlemmatize\u001b[49m(texts\u001b[38;5;241m.\u001b[39mdata[\u001b[38;5;241m0\u001b[39m]))  \u001b[38;5;66;03m# For some reason I occasionally have to run this several times before things start working.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m lemmatized_texts \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlist\u001b[39m(lemmatize(text)) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts\u001b[38;5;241m.\u001b[39mdata]\n\u001b[0;32m      5\u001b[0m lemmatized_texts[\u001b[38;5;241m3\u001b[39m][:\u001b[38;5;241m10\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'lemmatize' is not defined"
     ]
    }
   ],
   "source": [
    "# Here we start from full sentences again to help the NLP library understand the types of words. \n",
    "# Note: this can take quite a while!\n",
    "print(lemmatize(texts.data[0]))  # For some reason I occasionally have to run this several times before things start working.\n",
    "lemmatized_texts = [list(lemmatize(text)) for text in texts.data]\n",
    "lemmatized_texts[3][:10]  # Print a random example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ipxM0N0xQ8v"
   },
   "outputs": [],
   "source": [
    "# Create a corpus from a list of texts.\n",
    "# The dictionary just extracts and numbers each distinct word.\n",
    "dictionary = Dictionary(lemmatized_texts)\n",
    "# A corpus is a sparse datastore containing the number of times each word appears in each document.\n",
    "corpus = [dictionary.doc2bow(text) for text in lemmatized_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ieBsMmsUzihf"
   },
   "outputs": [],
   "source": [
    "# Print a sample of dictionary items.\n",
    "top_words_in_doc_0 = sorted(corpus[0], key=lambda e: e[1], reverse=True)[:10]\n",
    "for word_index, count in top_words_in_doc_0:\n",
    "  print(f'{dictionary[word_index]}   \\tindex: {word_index}\\tcount: {count:,}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cf5fKgmq6UWR"
   },
   "source": [
    "## Models Trained on Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_xj6G_Nm6WAc"
   },
   "outputs": [],
   "source": [
    "# Let's start with a simple TF-IDF model. Worth noting: SKLearn also has a Tfidf vectorizer.\n",
    "model = models.TfidfModel(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G4o_V5kl6w8L"
   },
   "outputs": [],
   "source": [
    "# What are the most _salient_ words in document 0?\n",
    "top_tfidf_words = sorted(model[corpus[0]], key=lambda e: e[1], reverse=True)[:10]\n",
    "for word_index, score in top_tfidf_words:\n",
    "  print(f'{dictionary[word_index]}   \\tindex: {word_index}\\ttf-idf: {score:.2f}')\n",
    "\n",
    "# What's different from the previous ranking? What do you think the topic label might be?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vlQrCIvd7QGD"
   },
   "outputs": [],
   "source": [
    "print(texts.target_names[texts.target[0]])  # 7 = 'rec.autos'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fSTedgQY9pjz"
   },
   "outputs": [],
   "source": [
    "# We could conver this to a giant vector spanning all documents and words.\n",
    "# It'll be mostly zeros, of course.\n",
    "tfidf_vector = np.zeros([len(texts.target), len(dictionary)])\n",
    "for doc_idx, doc in enumerate(corpus):\n",
    "  for word_ix, word_tfidf in model[doc]:\n",
    "    tfidf_vector[doc_idx, word_ix] = word_tfidf\n",
    "tfidf_vector.shape  # How many entries are there in this vector? What % do you guess is zero?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ePkxDwFHCRGz"
   },
   "outputs": [],
   "source": [
    "print(f'Percentage of 0 entries: {(tfidf_vector == 0).sum() / tfidf_vector.size:.3%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AJfsP5o176Ok"
   },
   "outputs": [],
   "source": [
    "# Then we can train a model that predicts the topic from these values.\n",
    "# A quick tangent: we'd normally use sklearn.feature_extraction.text.TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "clf = MultinomialNB(alpha=.01)\n",
    "clf.fit(tfidf_vector, texts.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YPuwmdMZAm_E"
   },
   "outputs": [],
   "source": [
    "# Let's index our test data in the same way and test the model.\n",
    "test_texts, test_targets = fetch_20newsgroups(subset='test', remove=('headers', 'footers', 'quotes'), return_X_y=True)\n",
    "# We'll save ourselves a bit of time by focusing on 1K documents.\n",
    "test_texts = test_texts[:1000]\n",
    "test_targets = test_targets[:1000]\n",
    "\n",
    "# Lemmatize text, index with the same dictionary, and create a TF-IDF vector.\n",
    "test_lemmatized = [list(lemmatize(text)) for text in test_texts]\n",
    "test_corpus = [dictionary.doc2bow(text) for text in test_lemmatized]\n",
    "test_tfidf_vector = np.zeros([len(test_targets), len(dictionary)])\n",
    "for doc_idx, doc in enumerate(test_corpus):\n",
    "  for word_ix, word_tfidf in model[doc]:\n",
    "    test_tfidf_vector[doc_idx, word_ix] = word_tfidf\n",
    "\n",
    "print(f'Accuracy: {clf.score(test_tfidf_vector, test_targets):.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pp8j6r5PCpQK"
   },
   "outputs": [],
   "source": [
    "# What are the most important features for each category?\n",
    "for ix, category in enumerate(texts.target_names):\n",
    "    top10 = np.argsort(clf.coef_[ix])[-10:]\n",
    "    print(f'{category}:   \\t{\" \".join([dictionary[index] for index in top10])}')\n",
    "# Pretty reasonable for the most part, but note the first few in almost every category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ntjeWXS7yTKd"
   },
   "outputs": [],
   "source": [
    "# Okay, let's do something more exciting: build an LDA model.\n",
    "# Note: we could also do this with SKLearn using LinearDiscriminantAnalysis.\n",
    "\n",
    "# Let's start with 20 topics.\n",
    "num_topics = 20\n",
    "model = models.LdaMulticore(corpus, num_topics=num_topics, id2word=dictionary, passes=10, workers=4, dtype=np.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QgjjmkzD7yyi"
   },
   "outputs": [],
   "source": [
    "# Let's again look at the most salient words per topic, but note that we no longer\n",
    "# have \"labels\" now.\n",
    "for ix in range(num_topics):\n",
    "  top10 = np.argsort(model.get_topics()[ix])[-10:]\n",
    "  print(f'{ix}:  {\" \".join([dictionary[index] for index in top10])}')  # See any patterns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s6fpoQghHGCh"
   },
   "outputs": [],
   "source": [
    "# Is this useful? One thing you can now do is find relevant neighbors for a given document\n",
    "def topics_to_vector(topics):\n",
    "  \"\"\"Little util method for creating vectors.\"\"\"\n",
    "  vector = np.zeros(num_topics)\n",
    "  for index, score in topics:\n",
    "    vector[index] = score\n",
    "  return vector\n",
    "\n",
    "# Print the first document with its category.\n",
    "print('Label:', texts.target_names[texts.target[0]])\n",
    "print(texts.data[0])\n",
    "print('\\n\\n')\n",
    "\n",
    "# Conver to a vector and find nearest neighbors.\n",
    "min_dist = 1e9\n",
    "nearest_neighbor = -1\n",
    "topic_vector = topics_to_vector(model.get_document_topics(corpus[0]))\n",
    "print(topic_vector)\n",
    "for ix, doc in enumerate(corpus[1:], start=1):\n",
    "  doc_vector = topics_to_vector(model.get_document_topics(doc))\n",
    "  dist = np.linalg.norm(topic_vector - doc_vector)\n",
    "  if dist < min_dist:\n",
    "    min_dist = dist\n",
    "    nearest_neighbor = ix\n",
    "    print(f'New nearest neighbor: {ix} with distance {dist:.4f} and label {texts.target_names[texts.target[ix]]}')\n",
    "\n",
    "print('\\n\\nNearest neighbor: ')\n",
    "print('Label:', texts.target_names[texts.target[nearest_neighbor]])\n",
    "print(texts.data[nearest_neighbor])\n",
    "print(topics_to_vector(model.get_document_topics(corpus[nearest_neighbor])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5AZW_u606Cdm"
   },
   "outputs": [],
   "source": [
    "# How coherent are these topics?\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "cm = CoherenceModel(model=model, texts=lemmatized_texts, dictionary=dictionary, coherence='u_mass')\n",
    "coherence = cm.get_coherence()\n",
    "coherence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oRjMvC9t6wBj"
   },
   "outputs": [],
   "source": [
    "# How about HAC?\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "cluster = AgglomerativeClustering(n_clusters=20, affinity='euclidean', linkage='single', compute_distances=True)\n",
    "cluster.fit(tfidf_vector[:2000])  # Note: this takes a very long time, so just using a small subset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rZMDuLbe7NbU"
   },
   "outputs": [],
   "source": [
    "# Let's visualize the clusters.\n",
    "# Using code from https://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_dendrogram.html#sphx-glr-auto-examples-cluster-plot-agglomerative-dendrogram-py\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "\n",
    "\n",
    "def plot_dendrogram(model, **kwargs):\n",
    "    \"\"\"Create linkage matrix and then plot the dendrogram.\"\"\"\n",
    "\n",
    "    # create the counts of samples under each node\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    linkage_matrix = np.column_stack(\n",
    "        [model.children_, model.distances_, counts]\n",
    "    ).astype(float)\n",
    "\n",
    "    # Plot the corresponding dendrogram\n",
    "    dendrogram(linkage_matrix, **kwargs)\n",
    "\n",
    "# setting distance_threshold=0 ensures we compute the full tree.\n",
    "plt.title(\"Hierarchical Clustering Dendrogram\")\n",
    "# plot the top three levels of the dendrogram\n",
    "plot_dendrogram(cluster, truncate_mode=\"level\", p=20)\n",
    "plt.xlabel(\"Number of points in node (or index of point if no parenthesis).\")\n",
    "plt.show()\n",
    "# Not very informative; might just require more samples."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
